{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ca5828",
   "metadata": {},
   "source": [
    "# Learn basics in NLP with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e7aa5",
   "metadata": {},
   "source": [
    "I'm gonna follow this github tutorial.\n",
    "\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e838",
   "metadata": {},
   "source": [
    "Get dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataaset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9fd29",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_lables, val_lables = train_test_split(\n",
    "    train_data[\"text\"].to_numpy(),\n",
    "    train_data[\"target\"].to_numpy(),\n",
    "    test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71583705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Simmering beneath #NHL good times the league's own concussion issues @PioneerPress\\n\\nhttp://t.co/zl7FhUCxHL\",\n",
       "       'U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/MatIJwkzbh via @Change',\n",
       "       '#world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps  http://t.co/wvExJjRG6E',\n",
       "       ...,\n",
       "       'Hollywood movie about trapped miners released in #Chile http://t.co/r18aUtnLSd #ZippedNews http://t.co/CNqaE9foj6',\n",
       "       '*New!* Stretcher in 5 min https://t.co/q5MDsNbCMh (by FUJIWARA Shunichiro 2015-08-05) [Technology]',\n",
       "       'Barak will Tell the American People that the lives of the Hostages in Iran depends on Congress Voting to give Terrorist a Nuke for hostages.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2b5db",
   "metadata": {},
   "source": [
    "# Converting text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eda15",
   "metadata": {},
   "source": [
    "Create words to vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f4054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = TextVectorization(\n",
    "    max_tokens=10000, standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode='int',\n",
    "    output_sequence_length=None, pad_to_max_tokens=False, vocabulary=None,\n",
    "    idf_weights=None, sparse=False, ragged=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c471c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbba5",
   "metadata": {},
   "source": [
    "See how the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762f78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=int64, numpy=array([[ 73,   9,   3, 218,   4,  13, 668]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There is a flood in my street!\"\n",
    "text2vec([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531229f",
   "metadata": {},
   "source": [
    "Get first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158be6",
   "metadata": {},
   "source": [
    "Get the words from 100 to 105th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fb4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'bomb', 'first', 'world', 'see']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c48a",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9743e",
   "metadata": {},
   "source": [
    "We are going to use TnsorFlow's embedding layers.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a94a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7f4294225d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = 10000, # set imput shape\n",
    "                             output_dim = 128, # output shape\n",
    "                             input_length = 10000 # how long is each input \n",
    "                            )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082231d8",
   "metadata": {},
   "source": [
    "Get a random sentence from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2839bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " I came up with an idea of a fragrance concept for a bath bomb called The Blood of my Enemies. So you can say that's what you bathe in.        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 29, 128), dtype=float32, numpy=\n",
       "array([[[ 0.02958575, -0.01597808, -0.01917583, ...,  0.03905093,\n",
       "         -0.02408947, -0.01861982],\n",
       "        [ 0.02632899, -0.0480075 , -0.0182404 , ...,  0.0014418 ,\n",
       "          0.00921815,  0.03629432],\n",
       "        [-0.04899255, -0.0412196 , -0.01272858, ...,  0.04235465,\n",
       "          0.0411538 , -0.04494948],\n",
       "        ...,\n",
       "        [-0.01053566,  0.0204275 ,  0.00317945, ..., -0.03147644,\n",
       "         -0.0228538 ,  0.03613024],\n",
       "        [-0.03692415, -0.00882105,  0.01451446, ...,  0.01105335,\n",
       "          0.03074242, -0.00395235],\n",
       "        [-0.01825533, -0.03438073,  0.03379628, ...,  0.00583144,\n",
       "         -0.00925924,  0.02018284]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text2vec([random_sentence]))\n",
    "sample_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6b3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([ 2.95857452e-02, -1.59780756e-02, -1.91758275e-02, -4.34473418e-02,\n",
       "        -4.15468924e-02,  9.74230841e-03,  3.61525752e-02, -3.13725844e-02,\n",
       "         3.18538025e-03, -9.12852213e-03,  2.62330882e-02,  1.42868869e-02,\n",
       "         2.21077688e-02,  1.76996700e-02, -3.93583290e-02, -4.68223095e-02,\n",
       "         1.66739561e-02, -4.23863307e-02, -2.55356561e-02,  1.80129670e-02,\n",
       "        -2.92800423e-02,  4.26120795e-02,  4.11627777e-02,  1.35728456e-02,\n",
       "        -4.69775461e-02, -4.24849652e-02,  3.65714468e-02, -3.03423051e-02,\n",
       "         1.49317123e-02,  2.76662447e-02,  2.05638073e-02, -3.21891159e-03,\n",
       "         9.80960205e-03,  2.10213661e-03, -3.11073661e-02, -1.41449086e-02,\n",
       "        -3.63717563e-02, -4.56285849e-02,  1.92188136e-02,  2.33934857e-02,\n",
       "         2.31421329e-02, -6.20067120e-04, -2.98727993e-02, -9.18539613e-03,\n",
       "         3.26558091e-02,  2.99807452e-02, -2.63557434e-02, -1.16965771e-02,\n",
       "         3.78444232e-02,  4.54924367e-02, -3.20964940e-02,  4.50732149e-02,\n",
       "        -3.79822478e-02, -2.32872013e-02,  3.16943415e-02,  2.55504586e-02,\n",
       "         1.54930018e-02,  1.67221688e-02, -3.32309753e-02, -1.10343695e-02,\n",
       "        -4.11925428e-02, -3.39025743e-02, -4.54010256e-02, -1.61281824e-02,\n",
       "         4.34077866e-02,  2.73547657e-02, -5.83446026e-03, -4.79758866e-02,\n",
       "         7.82655552e-03, -2.17152834e-02, -2.24405658e-02, -2.50382908e-02,\n",
       "        -2.79778726e-02,  4.46416996e-02, -4.23969515e-02,  4.13106345e-02,\n",
       "        -4.19087522e-02,  4.74279039e-02, -3.11381742e-03, -4.03807312e-03,\n",
       "        -4.37123440e-02, -3.40795144e-02, -2.98872348e-02, -4.02458906e-02,\n",
       "        -1.81805491e-02,  2.44003050e-02,  3.29891592e-03,  1.71298496e-02,\n",
       "         4.70840968e-02,  3.74727324e-03,  3.53443958e-02,  1.80848353e-02,\n",
       "         4.22977246e-02, -3.90703082e-02,  4.85027768e-02, -1.28516331e-02,\n",
       "         6.19614124e-03,  1.71411671e-02, -3.01180482e-02, -3.79459374e-02,\n",
       "         1.74337886e-02,  3.87961753e-02,  4.07869481e-02,  4.26831581e-02,\n",
       "         4.03702743e-02,  1.33732222e-02,  2.64021493e-02, -4.37751785e-02,\n",
       "         4.05398048e-02, -5.08911908e-05,  7.32595846e-03, -4.71144207e-02,\n",
       "         4.17439975e-02,  1.15332380e-02, -4.80739735e-02, -3.89659405e-02,\n",
       "        -4.77622412e-02, -3.01694758e-02, -2.98504904e-03,  3.61027829e-02,\n",
       "        -2.70358678e-02, -4.52722684e-02, -2.78549083e-02,  1.07928365e-03,\n",
       "         2.44637467e-02,  3.90509255e-02, -2.40894686e-02, -1.86198242e-02],\n",
       "       dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " \"I came up with an idea of a fragrance concept for a bath bomb called The Blood of my Enemies. So you can say that's what you bathe in.\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06be48a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset with running a series of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c632",
   "metadata": {},
   "source": [
    "There are some Model to learn text:\n",
    "\n",
    "0, Naive Bayes with TF-IDF encoder (baseline)\n",
    "\n",
    "1, Feed-forward neural network (dence model)\n",
    "\n",
    "2, LSTM (RNN)\n",
    "\n",
    "3, GRU (RNN)\n",
    "\n",
    "4, Bidirectional-LSTM (RNN)\n",
    "\n",
    "5, 1D Convolutional Neural Network\n",
    "\n",
    "6, TensorFlow Hub Pretrained Feature Extractor\n",
    "\n",
    "7, TensorFlow Hub Pretrained Feature Extractor (10% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ceadf",
   "metadata": {},
   "source": [
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modeling with tensorflow:\n",
    "\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
