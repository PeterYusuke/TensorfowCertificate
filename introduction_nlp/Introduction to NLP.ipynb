{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ca5828",
   "metadata": {},
   "source": [
    "# Learn basics in NLP with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e7aa5",
   "metadata": {},
   "source": [
    "I'm gonna follow this github tutorial.\n",
    "\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e838",
   "metadata": {},
   "source": [
    "Get dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataaset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9fd29",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_lables, val_lables = train_test_split(\n",
    "    train_data[\"text\"].to_numpy(),\n",
    "    train_data[\"target\"].to_numpy(),\n",
    "    test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71583705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Since the chemical-weapons 'red line' warning on 20 August 2012 LCC have confirmed that at least 96355 people have been killed in #Syria.\",\n",
       "       '@iK4LEN Sirens was cancelled.',\n",
       "       'My brother-n-law riooooos got the call to head up north and fight the wild fires. Dudes a beast at\\x89Û_ https://t.co/463P0yS0Eb',\n",
       "       ...,\n",
       "       'George Njenga the hero saved his burning friend from a razing wildfire... http://t.co/us8r6Qsn0p',\n",
       "       'Omg earthquake',\n",
       "       'Goulburn man Henry Van Bilsen missing: Emergency services are searching for a Goulburn man who disappeared from his\\x89Û_ http://t.co/z99pKJzTRp'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2b5db",
   "metadata": {},
   "source": [
    "# Converting text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eda15",
   "metadata": {},
   "source": [
    "Create words to vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = TextVectorization(\n",
    "    max_tokens=10000, standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode='int',\n",
    "    output_sequence_length=None, pad_to_max_tokens=False, vocabulary=None,\n",
    "    idf_weights=None, sparse=False, ragged=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c471c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbba5",
   "metadata": {},
   "source": [
    "See how the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762f78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=int64, numpy=array([[ 75,   9,   3, 210,   4,  13, 693]], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There is a flood in my street!\"\n",
    "text2vec([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531229f",
   "metadata": {},
   "source": [
    "Get first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158be6",
   "metadata": {},
   "source": [
    "Get the words from 100 to 105th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fb4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why', 'going', 'see', 'day', 'love']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c48a",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9743e",
   "metadata": {},
   "source": [
    "We are going to use TnsorFlow's embedding layers.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a94a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x2003df72a30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = 10000, # set imput shape\n",
    "                             output_dim = 128, # output shape\n",
    "                             input_length = 10000 # how long is each input \n",
    "                            )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082231d8",
   "metadata": {},
   "source": [
    "Get a random sentence from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2839bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " 'I know a dill pickle when I taste one' -me        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 128), dtype=float32, numpy=\n",
       "array([[[-0.02135775, -0.03758646, -0.01996627, ...,  0.0278645 ,\n",
       "         -0.00598536, -0.03844702],\n",
       "        [-0.04226902, -0.00163647,  0.01667083, ..., -0.0428711 ,\n",
       "          0.01848867,  0.02290911],\n",
       "        [-0.03672609,  0.02936644,  0.03548683, ..., -0.00916771,\n",
       "          0.01369672,  0.02785199],\n",
       "        ...,\n",
       "        [ 0.01022422, -0.0036725 ,  0.03455767, ...,  0.04977309,\n",
       "          0.00265974,  0.03455759],\n",
       "        [-0.0054733 , -0.01895433,  0.01308021, ..., -0.01617257,\n",
       "         -0.0139548 , -0.03751872],\n",
       "        [-0.00618932, -0.0489864 , -0.04190475, ..., -0.03495146,\n",
       "          0.01029365,  0.035929  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text2vec([random_sentence]))\n",
    "sample_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c6b3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-2.13577505e-02, -3.75864618e-02, -1.99662689e-02, -4.48975675e-02,\n",
       "        -4.63089831e-02,  8.18821043e-03,  4.48398627e-02,  4.28785421e-02,\n",
       "         1.22422464e-02, -3.86800542e-02, -3.70315202e-02, -1.32973567e-02,\n",
       "         2.33544819e-02,  2.45478265e-02,  4.21095155e-02,  1.07404366e-02,\n",
       "         9.82768461e-03, -4.57753912e-02,  5.08897379e-03, -2.09103227e-02,\n",
       "         3.58012356e-02, -8.75191763e-03,  3.25902589e-02, -4.40030210e-02,\n",
       "        -2.79144049e-02, -2.47259866e-02,  1.21647716e-02,  7.70256668e-03,\n",
       "         3.79385240e-02, -2.14370247e-02, -1.16806030e-02,  2.45687254e-02,\n",
       "        -2.04440001e-02,  3.18035968e-02, -8.55914503e-03, -1.69825181e-02,\n",
       "         4.21337374e-02,  9.49386507e-03, -4.96903807e-03,  1.86784007e-02,\n",
       "         1.38633586e-02, -9.06940550e-03, -2.76816841e-02,  1.41907483e-04,\n",
       "         3.39682437e-02,  3.29332426e-03,  1.79452822e-03,  1.82428211e-03,\n",
       "         4.10582907e-02, -4.32386883e-02, -2.34949589e-02,  4.56622876e-02,\n",
       "        -3.86263244e-02, -1.56943873e-03,  1.15191936e-02, -4.91335876e-02,\n",
       "         1.64566897e-02,  2.49242075e-02,  4.87599522e-03,  1.41752027e-02,\n",
       "         2.88327001e-02, -1.98470000e-02, -4.46746126e-02,  4.44368608e-02,\n",
       "         9.65975225e-04,  1.19226351e-02, -1.69945471e-02,  4.48935889e-02,\n",
       "        -3.76562849e-02,  3.72101702e-02, -1.16485953e-02, -3.56292725e-03,\n",
       "         2.86694281e-02, -1.45663135e-02,  4.16658074e-03, -4.71088178e-02,\n",
       "        -4.94830310e-05,  2.47001760e-02,  4.79084589e-02, -3.43759432e-02,\n",
       "         2.10728683e-02, -1.17843524e-02,  1.05267540e-02,  3.33938636e-02,\n",
       "         1.28799938e-02, -4.80655804e-02, -1.18203163e-02, -4.08094749e-02,\n",
       "         2.33439915e-02,  4.57854532e-02,  7.61793926e-03,  4.29651178e-02,\n",
       "         1.80672295e-02,  4.94199991e-03,  4.21595909e-02,  2.82667764e-02,\n",
       "        -4.17910106e-02,  4.76736464e-02,  2.64753141e-02,  4.23864163e-02,\n",
       "         4.83698770e-03, -3.38709354e-02,  3.09023149e-02, -2.88915392e-02,\n",
       "         2.50660293e-02, -2.96315085e-02, -2.11768281e-02, -4.73024696e-03,\n",
       "        -3.43573689e-02,  2.21685320e-03,  5.18313795e-03,  4.13448252e-02,\n",
       "         4.05257680e-02, -3.23743746e-03,  3.36858369e-02,  1.84548534e-02,\n",
       "         4.44758683e-04, -3.79612334e-02,  8.76253843e-03,  3.76049615e-02,\n",
       "         2.62672193e-02,  2.56408006e-04, -2.52349377e-02,  2.63767131e-02,\n",
       "        -2.31144670e-02,  2.78645046e-02, -5.98535687e-03, -3.84470224e-02],\n",
       "       dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " \"'I know a dill pickle when I taste one' -me\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06be48a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset with running a series of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c632",
   "metadata": {},
   "source": [
    "There are some Model to learn text:\n",
    "\n",
    "0, Naive Bayes with TF-IDF encoder (baseline)\n",
    "\n",
    "1, Feed-forward neural network (dence model)\n",
    "\n",
    "2, LSTM (RNN)\n",
    "\n",
    "3, GRU (RNN)\n",
    "\n",
    "4, Bidirectional-LSTM (RNN)\n",
    "\n",
    "5, 1D Convolutional Neural Network\n",
    "\n",
    "6, TensorFlow Hub Pretrained Feature Extractor\n",
    "\n",
    "7, TensorFlow Hub Pretrained Feature Extractor (10% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ceadf",
   "metadata": {},
   "source": [
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modeling with tensorflow:\n",
    "\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ef0ca",
   "metadata": {},
   "source": [
    "## Model 0 : Naive Bayes with TF-IDF encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740eec42",
   "metadata": {},
   "source": [
    "This is famouse when we don't use DL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e9e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model_0 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", MultinomialNB()) # model the text\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65f1c18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af9906f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8044619422572179"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_lables)\n",
    "baseline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bae54e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict model\n",
    "baseline_pre = model_0.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3cf53",
   "metadata": {},
   "source": [
    "Evaluate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aebf3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import caluculate_results\n",
    "baseline_results =  caluculate_results(y_true = val_lables,\n",
    "                                       y_pre = baseline_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecfc3270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 80.4461942257218,\n",
       " 'prediction': 0.8156975938097631,\n",
       " 'recall': 0.8044619422572179,\n",
       " 'f1': 0.7977634834805553}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc478bd",
   "metadata": {},
   "source": [
    "# Model 1: simple dence layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
