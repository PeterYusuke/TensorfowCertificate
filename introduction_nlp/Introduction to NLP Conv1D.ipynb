{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ca5828",
   "metadata": {},
   "source": [
    "# Learn basics in NLP with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e7aa5",
   "metadata": {},
   "source": [
    "I'm gonna follow this github tutorial.\n",
    "\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e838",
   "metadata": {},
   "source": [
    "Get dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataaset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9fd29",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_lables, val_lables = train_test_split(\n",
    "    train_data[\"text\"].to_numpy(),\n",
    "    train_data[\"target\"].to_numpy(),\n",
    "    test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71583705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@okgabby_ damn suh. don't let that ruin your year bruh. this our year. better start carpooling like we did back in the day\",\n",
       "       \"'If you are going to achieve excellence in big things you develop the habit in little matters....' dont know the author\",\n",
       "       'Choking Hazard Prompts Recall Of Kraft Cheese Singles http://t.co/XGKyVF9t4f',\n",
       "       ...,\n",
       "       'Aquarium Ornament Wreck Sailing Boat Sunk Ship Destroyer Fish Tank Cave Decor - Full read \\x89Û_ http://t.co/nosA8JJjiN http://t.co/WUKvdavUJu',\n",
       "       'Bluedio Turbine Hurricane H Bluetooth 4.1 Wireless Stereo Headphones Headset BLK - Full re\\x89Û_ http://t.co/WeUDLkc4o4 http://t.co/trl1dskF81',\n",
       "       \"http://t.co/XlFi7ovhFJ VIDEO: 'We're picking up bodies from water': Rescuers are searching for hundreds\\x89Û_ http://t.co/rAq4ZpdvKe\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2b5db",
   "metadata": {},
   "source": [
    "# Converting text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eda15",
   "metadata": {},
   "source": [
    "Create words to vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = TextVectorization(\n",
    "    max_tokens=10000, standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode='int',\n",
    "    output_sequence_length=15, pad_to_max_tokens=False, vocabulary=None,\n",
    "    idf_weights=None, sparse=False, ragged=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c471c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbba5",
   "metadata": {},
   "source": [
    "See how the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762f78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 72,   9,   3, 216,   5,  13, 701,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There is a flood in my street!\"\n",
    "text2vec([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531229f",
   "metadata": {},
   "source": [
    "Get first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'to']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158be6",
   "metadata": {},
   "source": [
    "Get the words from 100 to 105th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fb4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'first', 'got', 'world', 'love']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c48a",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9743e",
   "metadata": {},
   "source": [
    "We are going to use TnsorFlow's embedding layers.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a94a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x286893a7dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = 10000, # set imput shape\n",
    "                             output_dim = 128, # output shape\n",
    "                             input_length = 10000 # how long is each input \n",
    "                            )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082231d8",
   "metadata": {},
   "source": [
    "Get a random sentence from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2839bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Boy saves autistic brother from drowning: A nine-year-old in Maine dove into a pool to save his autistic brother from drowning        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.01633505, -0.0333326 ,  0.02428682, ...,  0.03221834,\n",
       "          0.02810869, -0.03226112],\n",
       "        [-0.0431253 ,  0.04908233,  0.01159443, ...,  0.04121036,\n",
       "         -0.04984248,  0.02418424],\n",
       "        [ 0.03160769,  0.04385887, -0.0315024 , ...,  0.00981873,\n",
       "         -0.04838436,  0.01833885],\n",
       "        ...,\n",
       "        [-0.02509712,  0.00252758,  0.01120732, ...,  0.03815761,\n",
       "          0.02797906, -0.00581179],\n",
       "        [-0.01354187,  0.01167788,  0.03739823, ..., -0.02990412,\n",
       "         -0.00281852, -0.00471834],\n",
       "        [ 0.02459984, -0.03324062, -0.03261051, ..., -0.02778139,\n",
       "         -0.01932004, -0.03199853]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text2vec([random_sentence]))\n",
    "sample_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6b3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.01633505, -0.0333326 ,  0.02428682, -0.01172941,  0.01253531,\n",
       "        -0.02390733, -0.01692311, -0.02879509,  0.02773492,  0.0234932 ,\n",
       "         0.04054533,  0.03897861,  0.00245571,  0.02664149, -0.02715945,\n",
       "        -0.0043594 , -0.0085379 ,  0.00631136,  0.01319552,  0.00289512,\n",
       "         0.0367268 , -0.01734496, -0.02654569,  0.01388904, -0.04451586,\n",
       "        -0.00824375,  0.00538548,  0.02327209,  0.0431522 , -0.04723018,\n",
       "        -0.04571656, -0.04024692, -0.0012235 ,  0.00289857,  0.02234027,\n",
       "        -0.01382188,  0.00131958, -0.02938548, -0.0448084 ,  0.00228596,\n",
       "        -0.04224558, -0.02415817,  0.00232942,  0.01215092, -0.02803907,\n",
       "         0.04449954,  0.03510661, -0.00244106,  0.02183453,  0.02190504,\n",
       "         0.01023483,  0.01911161, -0.02444285,  0.0303578 ,  0.04145867,\n",
       "        -0.0187322 ,  0.01889959,  0.01434069, -0.03980677, -0.01571401,\n",
       "         0.04318125, -0.04801739, -0.01193171, -0.04218401, -0.04322013,\n",
       "        -0.04984198, -0.0219878 , -0.02651212, -0.04084063,  0.02454522,\n",
       "         0.02363826, -0.01525735, -0.01777915,  0.02168718,  0.0221634 ,\n",
       "         0.0279714 ,  0.03833677,  0.00755138, -0.03465331,  0.01533569,\n",
       "        -0.03383843, -0.01986117,  0.00504475,  0.043393  , -0.03819514,\n",
       "         0.00613238, -0.0195007 ,  0.00325381,  0.00748207, -0.03657198,\n",
       "        -0.01231628,  0.02557368, -0.01637665, -0.00728196, -0.03025637,\n",
       "         0.0183728 ,  0.02957398, -0.04825759, -0.03761715, -0.00710497,\n",
       "        -0.04436245,  0.04806599, -0.01340401,  0.01917528,  0.00607346,\n",
       "        -0.01552124, -0.02344009,  0.04702493,  0.03127683, -0.02805724,\n",
       "         0.01345009,  0.01331932,  0.02590357,  0.03875573, -0.04416044,\n",
       "         0.00688137, -0.04344004,  0.01764106,  0.003818  , -0.0038114 ,\n",
       "        -0.03129126, -0.04091171,  0.01108688, -0.04527105, -0.04060149,\n",
       "         0.03221834,  0.02810869, -0.03226112], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 'Boy saves autistic brother from drowning: A nine-year-old in Maine dove into a pool to save his autistic brother from drowning')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06be48a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset with running a series of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c632",
   "metadata": {},
   "source": [
    "There are some Model to learn text:\n",
    "\n",
    "0, Naive Bayes with TF-IDF encoder (baseline)\n",
    "\n",
    "1, Feed-forward neural network (dence model)\n",
    "\n",
    "2, LSTM (RNN)\n",
    "\n",
    "3, GRU (RNN)\n",
    "\n",
    "4, Bidirectional-LSTM (RNN)\n",
    "\n",
    "5, 1D Convolutional Neural Network\n",
    "\n",
    "6, TensorFlow Hub Pretrained Feature Extractor\n",
    "\n",
    "7, TensorFlow Hub Pretrained Feature Extractor (10% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ceadf",
   "metadata": {},
   "source": [
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modeling with tensorflow:\n",
    "\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f8e51",
   "metadata": {},
   "source": [
    "# Create Conv1D layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54931a42",
   "metadata": {},
   "source": [
    "For more CNN info see below \n",
    "* https://poloclub.github.io/cnn-explainer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed80e10",
   "metadata": {},
   "source": [
    "When relu activation see below\n",
    "* https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14ff4c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test embedding layer, Conv1D layer and max pooling\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding_test = embedding(text2vec([\"This is a test sentence\"])) # trurn target sequence into embedding\n",
    "conv_1d = layers.Conv1D(filters=32,\n",
    "                       kernel_size=5,\n",
    "                       activation=\"relu\",\n",
    "                       padding=\"valid\")  # when \"valid\" the output size is smaller than input, when \"same\" the output size is the same\n",
    "conv_1d_output = conv_1d(embedding_test)  # pass test embedding through conv1d layer\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output)   # equivalent to \"get the most important feature\" or \"get the feature the highest value\"\n",
    "\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "798bc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback ( need to a new one for each model)\n",
    "from helper_function import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f31ff",
   "metadata": {},
   "source": [
    "# Create Conv1D layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1a71db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype = tf.string)\n",
    "x = text2vec(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\", padding=\"valid\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, output, name=\"model_5_Conv1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20193836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 15)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,321,089\n",
      "Trainable params: 1,321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7d4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_5.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c20dc1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/Conv1D/20220102-092036\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.5528 - accuracy: 0.7287 - val_loss: 0.4592 - val_accuracy: 0.7677\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.3341 - accuracy: 0.8612 - val_loss: 0.4779 - val_accuracy: 0.7743\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.2029 - accuracy: 0.9275 - val_loss: 0.5512 - val_accuracy: 0.7782\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1315 - accuracy: 0.9569 - val_loss: 0.6386 - val_accuracy: 0.7638\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0936 - accuracy: 0.9691 - val_loss: 0.7271 - val_accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(x=train_sentences,\n",
    "                             y=train_lables,\n",
    "                             epochs=5,\n",
    "                             validation_data=(val_sentences, val_lables),\n",
    "                             callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                   \"Conv1D\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be06eef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7710103 ],\n",
       "       [0.01703349],\n",
       "       [0.99977785],\n",
       "       [0.00562745],\n",
       "       [0.48096937],\n",
       "       [0.99865043],\n",
       "       [0.923563  ],\n",
       "       [1.        ],\n",
       "       [0.94818616],\n",
       "       [0.9999392 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6974d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 0., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the model prediction to lables format\n",
    "model_5_pred = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1963e2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 74.1469816272966,\n",
       " 'prediction': 0.7413901742830225,\n",
       " 'recall': 0.7414698162729659,\n",
       " 'f1': 0.7414287125560604}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Evaluation import caluculate_results\n",
    "\n",
    "caluculate_results(y_true=val_lables,\n",
    "                  y_pre=model_5_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
