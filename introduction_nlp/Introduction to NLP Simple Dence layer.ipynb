{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ca5828",
   "metadata": {},
   "source": [
    "# Learn basics in NLP with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e7aa5",
   "metadata": {},
   "source": [
    "I'm gonna follow this github tutorial.\n",
    "\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e838",
   "metadata": {},
   "source": [
    "Get dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataaset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9fd29",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_lables, val_lables = train_test_split(\n",
    "    train_data[\"text\"].to_numpy(),\n",
    "    train_data[\"target\"].to_numpy(),\n",
    "    test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71583705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'Congress' should be renamed Italian Goonda Party. They are a motley crowd of hooligans and selfavowed crooks determined to derail democracy\",\n",
       "       'Landslide kills three near Venice after heavyåÊrain http://t.co/q3Xq8R658r',\n",
       "       'Not one character in the final destination series has ever survived ??',\n",
       "       ...,\n",
       "       'The Witches of the Glass Castle. Supernatural YA where sibling rivalry magic and love collide #wogc #kindle http://t.co/IzakNpJeQW',\n",
       "       'Horrible Accident Man Died In Wings of Airplane (29-07-2015) http://t.co/TfcdRONRA6',\n",
       "       '@AdamTuss and is the car that derailed a 5000 series by chance. They used to have issues w/ wheel climbing RE: 1/2007 Mt. Vern Sq derailment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2b5db",
   "metadata": {},
   "source": [
    "# Converting text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eda15",
   "metadata": {},
   "source": [
    "Create words to vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = TextVectorization(\n",
    "    max_tokens=10000, standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode='int',\n",
    "    output_sequence_length=15, pad_to_max_tokens=False, vocabulary=None,\n",
    "    idf_weights=None, sparse=False, ragged=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c471c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbba5",
   "metadata": {},
   "source": [
    "See how the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762f78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 74,   9,   3, 224,   4,  13, 789,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There is a flood in my street!\"\n",
    "text2vec([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531229f",
   "metadata": {},
   "source": [
    "Get first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158be6",
   "metadata": {},
   "source": [
    "Get the words from 100 to 105th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fb4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['man', 'fires', 'world', 'rt', 'love']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c48a",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9743e",
   "metadata": {},
   "source": [
    "We are going to use TnsorFlow's embedding layers.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a94a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x1d6d86758e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = 10000, # set imput shape\n",
    "                             output_dim = 128, # output shape\n",
    "                             input_length = 10000 # how long is each input \n",
    "                            )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082231d8",
   "metadata": {},
   "source": [
    "Get a random sentence from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2839bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " ! Residents Return To Destroyed Homes As Washington Wildfire Burns on http://t.co/UcI8stQUg1        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.0337841 , -0.04265943, -0.01576056, ..., -0.04682023,\n",
       "         -0.01124186,  0.04004878],\n",
       "        [ 0.01670153,  0.00922741,  0.00738541, ..., -0.02125056,\n",
       "         -0.04204486, -0.0330943 ],\n",
       "        [-0.04058049,  0.0493963 , -0.00905965, ..., -0.01078407,\n",
       "         -0.03960771, -0.00675224],\n",
       "        ...,\n",
       "        [-0.04179449,  0.03868698, -0.01063225, ...,  0.00155853,\n",
       "          0.01839009,  0.035278  ],\n",
       "        [-0.04179449,  0.03868698, -0.01063225, ...,  0.00155853,\n",
       "          0.01839009,  0.035278  ],\n",
       "        [-0.04179449,  0.03868698, -0.01063225, ...,  0.00155853,\n",
       "          0.01839009,  0.035278  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text2vec([random_sentence]))\n",
    "sample_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6b3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([ 0.0337841 , -0.04265943, -0.01576056, -0.04015245,  0.04022225,\n",
       "        -0.01401131,  0.03750006, -0.03184675,  0.02951287, -0.01774532,\n",
       "         0.03969281, -0.01669383, -0.03744419,  0.00642474,  0.02990139,\n",
       "        -0.03153314,  0.04652533, -0.00644759, -0.01268534, -0.0338016 ,\n",
       "         0.03158467,  0.01509025, -0.01870737, -0.04347018,  0.01826001,\n",
       "        -0.00459472, -0.00907284,  0.04063076,  0.02000294, -0.00943081,\n",
       "        -0.01354191, -0.00212723, -0.02696894, -0.01937709,  0.00748347,\n",
       "         0.01899696, -0.02962916, -0.00429243,  0.0160205 ,  0.04642891,\n",
       "        -0.01777109,  0.0095791 , -0.0049394 ,  0.03092838,  0.02097987,\n",
       "         0.03320912, -0.01151361, -0.03555859,  0.00896908,  0.02704009,\n",
       "         0.03326956, -0.03886857,  0.03857337, -0.00783563, -0.02429022,\n",
       "        -0.01858344, -0.01822566,  0.02388005,  0.00076503,  0.00428661,\n",
       "         0.02374265,  0.03831632, -0.03448   ,  0.02039299,  0.04884393,\n",
       "         0.03254665,  0.03230461, -0.04066553, -0.04174516,  0.04498408,\n",
       "        -0.0481034 ,  0.01353239,  0.04259766,  0.00391269,  0.04485161,\n",
       "         0.01114889, -0.04386185, -0.0070882 , -0.00594542,  0.02465102,\n",
       "         0.04974421,  0.02025156, -0.02461219, -0.0467496 , -0.024042  ,\n",
       "         0.02739612, -0.03536376, -0.0012924 ,  0.01703325,  0.04933767,\n",
       "        -0.03569942, -0.0401613 ,  0.01680103, -0.03255881, -0.04923391,\n",
       "         0.00701486,  0.0248131 , -0.01322772, -0.04708548,  0.00509164,\n",
       "        -0.01693339, -0.02611886, -0.01249578, -0.01356151,  0.00366474,\n",
       "        -0.01699454,  0.0455503 , -0.01866859,  0.01711253, -0.01482994,\n",
       "        -0.04944072,  0.01703695,  0.01236463, -0.03986715, -0.00903305,\n",
       "         0.03603348, -0.00226974, -0.04869144,  0.02559097,  0.01759619,\n",
       "        -0.04335283,  0.04084224, -0.03592981, -0.01481969, -0.0388091 ,\n",
       "        -0.04682023, -0.01124186,  0.04004878], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " '! Residents Return To Destroyed Homes As Washington Wildfire Burns on http://t.co/UcI8stQUg1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06be48a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset with running a series of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c632",
   "metadata": {},
   "source": [
    "There are some Model to learn text:\n",
    "\n",
    "0, Naive Bayes with TF-IDF encoder (baseline)\n",
    "\n",
    "1, Feed-forward neural network (dence model)\n",
    "\n",
    "2, LSTM (RNN)\n",
    "\n",
    "3, GRU (RNN)\n",
    "\n",
    "4, Bidirectional-LSTM (RNN)\n",
    "\n",
    "5, 1D Convolutional Neural Network\n",
    "\n",
    "6, TensorFlow Hub Pretrained Feature Extractor\n",
    "\n",
    "7, TensorFlow Hub Pretrained Feature Extractor (10% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ceadf",
   "metadata": {},
   "source": [
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modeling with tensorflow:\n",
    "\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ef0ca",
   "metadata": {},
   "source": [
    "## Model 1 : Simple Dence layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740eec42",
   "metadata": {},
   "source": [
    "Create simple dence layer prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28bab8",
   "metadata": {},
   "source": [
    "![Simple Dence Layer](img/SimpleDenceLayer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45fa2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback ( need to a new one for each model)\n",
    "from helper_function import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103681a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with thr Function API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # inputs are 1-dimentional strings\n",
    "x = text2vec(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the nuberized inputs\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Create output layer, want binary outputs so use sigmoid function \n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dence\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b59d6e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dence\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 15)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6a511",
   "metadata": {},
   "source": [
    "### compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "956bcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca1ca407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20211228-143317\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 15ms/step - loss: 0.6127 - accuracy: 0.6916 - val_loss: 0.5226 - val_accuracy: 0.7677\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.4432 - accuracy: 0.8174 - val_loss: 0.4523 - val_accuracy: 0.7953\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3479 - accuracy: 0.8586 - val_loss: 0.4393 - val_accuracy: 0.7966\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2841 - accuracy: 0.8914 - val_loss: 0.4522 - val_accuracy: 0.7900\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2368 - accuracy: 0.9129 - val_loss: 0.4718 - val_accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "# fir the model\n",
    "\n",
    "model_1_history = model_1.fit(x=train_sentences,\n",
    "                             y=train_lables,\n",
    "                             epochs=5,\n",
    "                             validation_data=(val_sentences, val_lables),\n",
    "                             callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                   experiment_name=\"model_1_dense\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85eca542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47177064418792725, 0.7887139320373535]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the results\n",
    "\n",
    "model_1.evaluate(val_sentences, val_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "693bfb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762, 1)\n",
      "[[0.09253025]\n",
      " [0.9972645 ]\n",
      " [0.02822798]\n",
      " [0.9914555 ]\n",
      " [0.036149  ]\n",
      " [0.9618006 ]\n",
      " [0.70454437]\n",
      " [0.992393  ]\n",
      " [0.37690672]\n",
      " [0.03268823]]\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions and evaluate those\n",
    "\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "print(model_1_pred_probs.shape)\n",
    "print(model_1_pred_probs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3ebb7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model prediction to label format\n",
    "\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12c847f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.87139107611549,\n",
       " 'prediction': 0.7929120493899015,\n",
       " 'recall': 0.7887139107611548,\n",
       " 'f1': 0.7868932145420301}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate our model_1 results \n",
    "from Evaluation import caluculate_results\n",
    "model_1_results = caluculate_results(y_true=val_lables, \n",
    "                                    y_pre=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93af379",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a68aa32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization layer\n",
    "words_in_vocab = text2vec.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b97fe085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.05605679,  0.05359774,  0.00250347, ...,  0.01564652,\n",
       "          0.03309035,  0.02445746],\n",
       "        [-0.01606945,  0.03757437,  0.04642731, ...,  0.02036933,\n",
       "         -0.02405428, -0.04277565],\n",
       "        [-0.0141153 ,  0.03720097,  0.05430916, ...,  0.06198725,\n",
       "         -0.01132409,  0.01451792],\n",
       "        ...,\n",
       "        [-0.08347785,  0.06697842,  0.07331058, ...,  0.03520257,\n",
       "          0.01218209, -0.01386334],\n",
       "        [ 0.03933527, -0.01829431, -0.08557785, ..., -0.11267728,\n",
       "         -0.08746089,  0.05525713],\n",
       "        [ 0.02558482, -0.09147273, -0.07978733, ..., -0.04099841,\n",
       "         -0.05591433,  0.09141265]], dtype=float32),\n",
       " (10000, 128))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "embed_weights, embed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a464bbb",
   "metadata": {},
   "source": [
    "The above number means, the every token are represented by a vector of 128 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc0b81",
   "metadata": {},
   "source": [
    "Read below url\n",
    "\n",
    "https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dab6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below is adapted from: \n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\"model_1\\cembedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"model_1\\embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "  if num == 0: \n",
    "     continue # skip padding token\n",
    "  vec = embed_weights[num]\n",
    "  out_m.write(word + \"\\n\") # write words to file\n",
    "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
