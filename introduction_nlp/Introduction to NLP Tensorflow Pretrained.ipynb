{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ca5828",
   "metadata": {},
   "source": [
    "# Learn basics in NLP with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e7aa5",
   "metadata": {},
   "source": [
    "I'm gonna follow this github tutorial.\n",
    "\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e838",
   "metadata": {},
   "source": [
    "Get dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataaset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9fd29",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_lables, val_lables = train_test_split(\n",
    "    train_data[\"text\"].to_numpy(),\n",
    "    train_data[\"target\"].to_numpy(),\n",
    "    test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71583705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Uber reduces drunk driving fatalities says independent study http://t.co/jVIVT6zrv7',\n",
       "       \"A little concerned about the number of forest fires where I'll be living\",\n",
       "       'descended or sunk however it may be to the shadowed land beyond the crest of a striking cobra landing harshly upon his back; torch and',\n",
       "       ...,\n",
       "       '[Comment] Deaths of older children: what do the data tell #US? http://t.co/p8Yr2po6Jn\\n #nghlth',\n",
       "       \"@astros stunningly poor defense it's not all on the pitcher. If our bats are MIA like the top of 1st inning this team is in trouble.\",\n",
       "       'Sinkhole leaking sewage opens in housing estate\\nIrish Independent-3 Aug 2015'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2b5db",
   "metadata": {},
   "source": [
    "# Converting text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eda15",
   "metadata": {},
   "source": [
    "Create words to vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = TextVectorization(\n",
    "    max_tokens=10000, standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode='int',\n",
    "    output_sequence_length=15, pad_to_max_tokens=False, vocabulary=None,\n",
    "    idf_weights=None, sparse=False, ragged=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c471c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbba5",
   "metadata": {},
   "source": [
    "See how the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762f78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 75,   9,   3, 213,   4,  13, 696,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There is a flood in my street!\"\n",
    "text2vec([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531229f",
   "metadata": {},
   "source": [
    "Get first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158be6",
   "metadata": {},
   "source": [
    "Get the words from 100 to 105th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fb4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buildings', 'cant', 'bomb', 'world', 'going']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c48a",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9743e",
   "metadata": {},
   "source": [
    "We are going to use TnsorFlow's embedding layers.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a94a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x286a6d90520>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = 10000, # set imput shape\n",
    "                             output_dim = 128, # output shape\n",
    "                             input_length = 10000 # how long is each input \n",
    "                            )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082231d8",
   "metadata": {},
   "source": [
    "Get a random sentence from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2839bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " #Sismo M 1.9 - 5km S of Volcano Hawaii: Time2015-08-06 01:04:01 UTC2015-08-05 15:04:01 -10:00 at ep... http://t.co/RTUeTdfBqb #CSismica        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.00110046, -0.01278925, -0.04345628, ..., -0.03667394,\n",
       "          0.04411669, -0.04826413],\n",
       "        [-0.00228111,  0.00094842, -0.033382  , ...,  0.01971699,\n",
       "         -0.0270062 , -0.00865058],\n",
       "        [-0.00937158,  0.03930024, -0.02984457, ...,  0.04780022,\n",
       "          0.03601365, -0.00774721],\n",
       "        ...,\n",
       "        [ 0.02725519, -0.01262987,  0.01173335, ..., -0.01251786,\n",
       "         -0.02179431, -0.00612444],\n",
       "        [ 0.02651985,  0.02771715, -0.03794049, ..., -0.04656031,\n",
       "          0.00767535, -0.00552149],\n",
       "        [ 0.04439998, -0.01148913, -0.01172309, ..., -0.00683018,\n",
       "         -0.04659912, -0.0124477 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text2vec([random_sentence]))\n",
    "sample_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6b3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.00110046, -0.01278925, -0.04345628, -0.03915166, -0.00508578,\n",
       "        -0.00270509, -0.01162582,  0.00302484,  0.01827208, -0.01555096,\n",
       "        -0.01495733, -0.02727971,  0.03612879,  0.01345444,  0.02473441,\n",
       "         0.01068245, -0.04250053,  0.01603914, -0.04426651,  0.03726841,\n",
       "         0.02058747, -0.00264572,  0.03418631,  0.02683239, -0.01889637,\n",
       "         0.04407502, -0.02523658, -0.00983491, -0.02066312,  0.01693488,\n",
       "        -0.00597306, -0.01795416,  0.02507987, -0.03917861, -0.0427908 ,\n",
       "         0.03813349,  0.04971219, -0.00885048, -0.02547884, -0.02516809,\n",
       "        -0.03229671,  0.02315296, -0.0471476 ,  0.0072696 ,  0.01165967,\n",
       "        -0.01315971,  0.03781008, -0.04096187,  0.04793744, -0.01164491,\n",
       "         0.03358987,  0.00564047, -0.03524301,  0.02943058, -0.01222594,\n",
       "         0.0306639 , -0.03248478, -0.01927526, -0.04097341,  0.01935121,\n",
       "        -0.03554165, -0.01483761, -0.02703774, -0.00381593, -0.04680095,\n",
       "         0.00467334, -0.0112898 ,  0.04376509, -0.02369131,  0.01382586,\n",
       "         0.03119013, -0.00670872, -0.01703241, -0.02610611,  0.03011242,\n",
       "         0.02923174, -0.04950041,  0.0417497 , -0.03185692, -0.03490006,\n",
       "         0.00599414,  0.03150792, -0.00563706,  0.03556572,  0.01397468,\n",
       "        -0.00255742,  0.00671303,  0.01854363,  0.01202625,  0.04813068,\n",
       "        -0.00727712,  0.04614426, -0.01435218,  0.04306484, -0.01793617,\n",
       "        -0.04895234, -0.00471753,  0.01258517, -0.0272951 ,  0.0260261 ,\n",
       "        -0.02206235, -0.04658889, -0.02614663,  0.04752488, -0.04235836,\n",
       "         0.04781384,  0.03019996, -0.01859959, -0.02689425, -0.01088805,\n",
       "         0.00078582, -0.04685283, -0.02481333,  0.01260557, -0.02853583,\n",
       "         0.01554047,  0.03149395,  0.03277563, -0.02433367, -0.04963241,\n",
       "         0.02313555, -0.00979071, -0.00612412,  0.01628364, -0.02647342,\n",
       "        -0.03667394,  0.04411669, -0.04826413], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " '#Sismo M 1.9 - 5km S of Volcano Hawaii: Time2015-08-06 01:04:01 UTC2015-08-05 15:04:01 -10:00 at ep... http://t.co/RTUeTdfBqb #CSismica')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06be48a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset with running a series of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c632",
   "metadata": {},
   "source": [
    "There are some Model to learn text:\n",
    "\n",
    "0, Naive Bayes with TF-IDF encoder (baseline)\n",
    "\n",
    "1, Feed-forward neural network (dence model)\n",
    "\n",
    "2, LSTM (RNN)\n",
    "\n",
    "3, GRU (RNN)\n",
    "\n",
    "4, Bidirectional-LSTM (RNN)\n",
    "\n",
    "5, 1D Convolutional Neural Network\n",
    "\n",
    "6, TensorFlow Hub Pretrained Feature Extractor\n",
    "\n",
    "7, TensorFlow Hub Pretrained Feature Extractor (10% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ceadf",
   "metadata": {},
   "source": [
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modeling with tensorflow:\n",
    "\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f8e51",
   "metadata": {},
   "source": [
    "# Create Tensorflow Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791506b2",
   "metadata": {},
   "source": [
    "refer this model\n",
    "* https://tfhub.dev/google/universal-sentence-encoder/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6602524",
   "metadata": {},
   "source": [
    "This apploach takes lots time with local PC, so comment out these code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a439aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19b8ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "# embed_samples = embed([sample_sentence,\n",
    "#                        \"When you can the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "# print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model\n",
    "# sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "#                                        input_shape=[],\n",
    "#                                        dtype=tf.string,\n",
    "#                                        trainable=False,\n",
    "#                                        name=\"USE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model useing the Sequence \n",
    "# model_6 = tf.keras.Sequential([\n",
    "#     sentence_encoder_layer,\n",
    "#     layers.Dense(1, activation=\"sigmoid\")\n",
    "# ], name=\"model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile\n",
    "# model_6.compile()\n",
    "# model_6.summary()\n",
    "# model_6.fit()\n",
    "# model_6.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
