{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ca5828",
   "metadata": {},
   "source": [
    "# Learn basics in NLP with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e7aa5",
   "metadata": {},
   "source": [
    "I'm gonna follow this github tutorial.\n",
    "\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e838",
   "metadata": {},
   "source": [
    "Get dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataaset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9fd29",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_lables, val_lables = train_test_split(\n",
    "    train_data[\"text\"].to_numpy(),\n",
    "    train_data[\"target\"].to_numpy(),\n",
    "    test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71583705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Any disaster impairs mental health especially in vulnerable individuals... http://t.co/ZisuwLqRHf',\n",
       "       'PM Abe pledged to make every effort to seek a world without nuclear weapons. http://t.co/CBXnHhZ6kD',\n",
       "       \"Forsure back in the gym tomorrow. Body isn't even at 50%. Don't wanna risk injuries.\",\n",
       "       ...,\n",
       "       \"Love how I don't get in any trouble for having people over and the house still being trashed\",\n",
       "       'Do you feel engulfed with low self-image? Take the quiz: http://t.co/YzDmouXQBO http://t.co/PeXfgawrG1',\n",
       "       \"I understand you wanting to hang out with your guy friends I'll give you your space but don't ruin my trust with you.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2b5db",
   "metadata": {},
   "source": [
    "# Converting text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eda15",
   "metadata": {},
   "source": [
    "Create words to vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4054ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 11:12:51.867819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-02 11:12:51.899593: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-01-02 11:12:51.899614: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-01-02 11:12:51.899898: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "text2vec = TextVectorization(\n",
    "    max_tokens=10000, standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode='int',\n",
    "    output_sequence_length=15, pad_to_max_tokens=False, vocabulary=None,\n",
    "    idf_weights=None, sparse=False, ragged=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c471c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbba5",
   "metadata": {},
   "source": [
    "See how the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762f78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 72,   9,   3, 228,   4,  13, 734,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There is a flood in my street!\"\n",
    "text2vec([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531229f",
   "metadata": {},
   "source": [
    "Get first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158be6",
   "metadata": {},
   "source": [
    "Get the words from 100 to 105th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fb4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bomb', 'buildings', 'see', 'our', 'know']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c48a",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9743e",
   "metadata": {},
   "source": [
    "We are going to use TnsorFlow's embedding layers.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a94a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7f6da40802e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = 10000, # set imput shape\n",
    "                             output_dim = 128, # output shape\n",
    "                             input_length = 10000 # how long is each input \n",
    "                            )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082231d8",
   "metadata": {},
   "source": [
    "Get a random sentence from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2839bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Sydney Traffic HAZARD Oil spill - BANKSTOWN Stacey St at Wattle St #sydtraffic #trafficnetwork        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.03892614, -0.04641541,  0.04777295, ..., -0.0052767 ,\n",
       "         -0.00435907, -0.0036934 ],\n",
       "        [ 0.01728321,  0.03248573, -0.02044897, ..., -0.03938868,\n",
       "         -0.02776018, -0.03744652],\n",
       "        [-0.04819452, -0.03230914, -0.03411354, ..., -0.00883552,\n",
       "         -0.03620319, -0.0242218 ],\n",
       "        ...,\n",
       "        [ 0.00067035,  0.03130225,  0.02520123, ..., -0.0402687 ,\n",
       "         -0.01895981,  0.03321929],\n",
       "        [ 0.02673699, -0.03297522,  0.02253493, ...,  0.04762368,\n",
       "          0.03784218, -0.04502017],\n",
       "        [ 0.02673699, -0.03297522,  0.02253493, ...,  0.04762368,\n",
       "          0.03784218, -0.04502017]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text2vec([random_sentence]))\n",
    "sample_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6b3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.03892614, -0.04641541,  0.04777295, -0.00414192,  0.018943  ,\n",
       "        -0.04670976, -0.03457719, -0.00282471,  0.00265484, -0.0055011 ,\n",
       "        -0.02693317,  0.03918287,  0.03043142, -0.00431378, -0.03164884,\n",
       "        -0.03541088, -0.04157429,  0.02756966, -0.02426277,  0.01664637,\n",
       "         0.00429921,  0.03090943,  0.04107935,  0.04609969,  0.00375288,\n",
       "         0.01543416, -0.00789005, -0.03775101,  0.04337377, -0.01977952,\n",
       "        -0.04641226, -0.0022339 , -0.01215573, -0.01021969, -0.00839522,\n",
       "         0.01770103,  0.02704338,  0.03156639,  0.02284536,  0.04737722,\n",
       "         0.02939064,  0.03326209, -0.03806438,  0.0435033 , -0.0443492 ,\n",
       "         0.01927627, -0.01093759,  0.03253   , -0.00324621,  0.00061215,\n",
       "         0.04780847, -0.01315317, -0.03231405, -0.01922563,  0.03342343,\n",
       "         0.0157779 ,  0.03171125,  0.04932895,  0.03321553,  0.0320182 ,\n",
       "         0.03938054,  0.0369794 , -0.02484522,  0.04967489, -0.01517323,\n",
       "        -0.02854331, -0.02389118, -0.02059122, -0.02327887,  0.0445886 ,\n",
       "        -0.04767981, -0.00063302, -0.00118996,  0.04564777, -0.0304639 ,\n",
       "         0.02060262,  0.04775974, -0.02447208,  0.01115037, -0.01135963,\n",
       "        -0.00237022,  0.03611663, -0.03525622,  0.02157055, -0.00201446,\n",
       "        -0.02066106,  0.01835555, -0.0347309 ,  0.00821271, -0.00605942,\n",
       "        -0.02387857, -0.04167728,  0.04305239, -0.0089864 , -0.02710518,\n",
       "        -0.04259573, -0.03835203,  0.02296264,  0.01730598,  0.02643908,\n",
       "         0.01712606, -0.0314662 ,  0.00359433, -0.0073716 ,  0.01190964,\n",
       "         0.00684674,  0.0472616 ,  0.03282918, -0.03192823,  0.00966314,\n",
       "         0.01859745, -0.00187997, -0.03746371, -0.04381821, -0.00789798,\n",
       "         0.03310256, -0.04762045, -0.01838956,  0.0408153 , -0.01762042,\n",
       "         0.04777277, -0.03887975, -0.01840926, -0.04584018, -0.04279207,\n",
       "        -0.0052767 , -0.00435907, -0.0036934 ], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 'Sydney Traffic HAZARD Oil spill - BANKSTOWN Stacey St at Wattle St #sydtraffic #trafficnetwork')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06be48a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset with running a series of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c632",
   "metadata": {},
   "source": [
    "There are some Model to learn text:\n",
    "\n",
    "0, Naive Bayes with TF-IDF encoder (baseline)\n",
    "\n",
    "1, Feed-forward neural network (dence model)\n",
    "\n",
    "2, LSTM (RNN)\n",
    "\n",
    "3, GRU (RNN)\n",
    "\n",
    "4, Bidirectional-LSTM (RNN)\n",
    "\n",
    "5, 1D Convolutional Neural Network\n",
    "\n",
    "6, TensorFlow Hub Pretrained Feature Extractor\n",
    "\n",
    "7, TensorFlow Hub Pretrained Feature Extractor (10% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ceadf",
   "metadata": {},
   "source": [
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modeling with tensorflow:\n",
    "\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f8e51",
   "metadata": {},
   "source": [
    "# Create Tensorflow Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791506b2",
   "metadata": {},
   "source": [
    "refer this model\n",
    "* https://tfhub.dev/google/universal-sentence-encoder/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6602524",
   "metadata": {},
   "source": [
    "This apploach takes lots time with local PC, so comment out these code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a439aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b8ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01602831  0.01068851  0.02425469 -0.01405769  0.01434426  0.08292625\n",
      "  0.01963368  0.06160142 -0.003527   -0.01216412  0.00978648 -0.01248495\n",
      "  0.01232345  0.09748451  0.06141113 -0.03728355  0.01860887 -0.04669856\n",
      "  0.00413912 -0.06363905 -0.024699    0.0271369   0.02284444 -0.00210028\n",
      " -0.00630594 -0.03964957  0.02220405  0.00115079 -0.03132173  0.00119527\n",
      " -0.04012548  0.04561892 -0.01530599 -0.00175915  0.02173127 -0.08450424\n",
      "  0.03340026  0.04604553 -0.02480252 -0.08681665  0.00702694 -0.00770478\n",
      " -0.01434541  0.07814164 -0.10676058 -0.05152994 -0.00858156 -0.03232234\n",
      " -0.03871094  0.02581467], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embed_samples = embed([sample_sentence,\n",
    "                       \"When you can the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab6a8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                       input_shape=[],\n",
    "                                       dtype=tf.string,\n",
    "                                       trainable=False,\n",
    "                                       name=\"USE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "574f20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model useing the Sequence \n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac932a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,798,337\n",
      "Trainable params: 513\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd291ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_6.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d1058c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback ( need to a new one for each model)\n",
    "from helper_function import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ab26afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_6_Pretrained/20220102-112717\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 7ms/step - loss: 0.6495 - accuracy: 0.7378 - val_loss: 0.6147 - val_accuracy: 0.7612\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5818 - accuracy: 0.7889 - val_loss: 0.5663 - val_accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5385 - accuracy: 0.7943 - val_loss: 0.5352 - val_accuracy: 0.7808\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5097 - accuracy: 0.7970 - val_loss: 0.5148 - val_accuracy: 0.7835\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4895 - accuracy: 0.7986 - val_loss: 0.5006 - val_accuracy: 0.7848\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_6_history = model_6.fit(x=train_sentences,\n",
    "                             y=train_lables,\n",
    "                             epochs=5,\n",
    "                             validation_data=(val_sentences, val_lables),\n",
    "                             callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                   \"model_6_Pretrained\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94b977ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40397117],\n",
       "       [0.860996  ],\n",
       "       [0.37137312],\n",
       "       [0.24236357],\n",
       "       [0.5873279 ],\n",
       "       [0.7531559 ],\n",
       "       [0.26694798],\n",
       "       [0.8313131 ],\n",
       "       [0.66474056],\n",
       "       [0.370508  ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "model_6_pred_probs = model_6.predict(val_sentences)\n",
    "model_6_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1395be44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 0., 0., 1., 1., 0., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_pred = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47970718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.4776902887139,\n",
       " 'prediction': 0.7839507588245316,\n",
       " 'recall': 0.7847769028871391,\n",
       " 'f1': 0.783566315955472}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Evaluation import caluculate_results\n",
    "model_6_result = caluculate_results(y_true=val_lables,\n",
    "                                   y_pre=model_6_pred)\n",
    "model_6_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
