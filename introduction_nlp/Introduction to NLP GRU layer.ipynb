{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ca5828",
   "metadata": {},
   "source": [
    "# Learn basics in NLP with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e7aa5",
   "metadata": {},
   "source": [
    "I'm gonna follow this github tutorial.\n",
    "\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e838",
   "metadata": {},
   "source": [
    "Get dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataaset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9fd29",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_lables, val_lables = train_test_split(\n",
    "    train_data[\"text\"].to_numpy(),\n",
    "    train_data[\"target\"].to_numpy(),\n",
    "    test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71583705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MEN CRUSH EVERY FUCKING DAY???????????????????????????? http://t.co/Fs4y1c9mNf',\n",
       "       '@Blazing_Ben @PattyDs50 @gwfrazee @JoshuaAssaraf Not really. Sadly I have come to expect that from Obama.',\n",
       "       '@aphyr I\\x89Ûªve been following you this long\\x89Û_ Sunk cost fallacy or somethin\\x89Ûª',\n",
       "       ..., '@OfficialMqm you are terrorist',\n",
       "       '@Mmchale13 *tries to electrocute self with phone cord*',\n",
       "       '70 Years After Atomic Bombs Japan Still Struggles With War Past: The anniversary of the devastation wrought b... http://t.co/Targ56iGBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2b5db",
   "metadata": {},
   "source": [
    "# Converting text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eda15",
   "metadata": {},
   "source": [
    "Create words to vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = TextVectorization(\n",
    "    max_tokens=10000, standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode='int',\n",
    "    output_sequence_length=15, pad_to_max_tokens=False, vocabulary=None,\n",
    "    idf_weights=None, sparse=False, ragged=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c471c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbba5",
   "metadata": {},
   "source": [
    "See how the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762f78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 74,   9,   3, 215,   4,  13, 696,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There is a flood in my street!\"\n",
    "text2vec([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531229f",
   "metadata": {},
   "source": [
    "Get first words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158be6",
   "metadata": {},
   "source": [
    "Get the words from 100 to 105th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fb4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['day', 'first', 'cant', 'buildings', 'attack']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.get_vocabulary()[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c48a",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9743e",
   "metadata": {},
   "source": [
    "We are going to use TnsorFlow's embedding layers.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a94a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x2ce0ca15c70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = 10000, # set imput shape\n",
    "                             output_dim = 128, # output shape\n",
    "                             input_length = 10000 # how long is each input \n",
    "                            )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082231d8",
   "metadata": {},
   "source": [
    "Get a random sentence from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2839bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Where will the winds take my gypsy blood this time? http://t.co/66YVulIZbk        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.01654463, -0.02916887,  0.00402608, ...,  0.0486904 ,\n",
       "         -0.02967325, -0.01296461],\n",
       "        [-0.02516477,  0.04138014, -0.01501142, ...,  0.0399284 ,\n",
       "          0.04077123,  0.04924088],\n",
       "        [ 0.02981856,  0.0029608 ,  0.00556079, ..., -0.0150679 ,\n",
       "          0.03289032, -0.02314901],\n",
       "        ...,\n",
       "        [-0.00976108, -0.02075405,  0.02464462, ...,  0.01897743,\n",
       "          0.01493886,  0.01296253],\n",
       "        [-0.00976108, -0.02075405,  0.02464462, ...,  0.01897743,\n",
       "          0.01493886,  0.01296253],\n",
       "        [-0.00976108, -0.02075405,  0.02464462, ...,  0.01897743,\n",
       "          0.01493886,  0.01296253]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text2vec([random_sentence]))\n",
    "sample_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6b3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.01654463, -0.02916887,  0.00402608,  0.03156095, -0.01280761,\n",
       "         0.0102576 , -0.02029748,  0.04976146,  0.00745783, -0.04059462,\n",
       "         0.00715708,  0.027896  , -0.03073162, -0.0142231 ,  0.04000933,\n",
       "         0.03548099,  0.02797547, -0.0358911 , -0.02592714, -0.03627281,\n",
       "         0.01622288,  0.02645807,  0.04425314,  0.01021792,  0.02007533,\n",
       "         0.01331313,  0.0130183 , -0.01840711, -0.00985068, -0.03270923,\n",
       "         0.0340765 ,  0.00344791,  0.04997556,  0.00017343, -0.01647408,\n",
       "        -0.04760018, -0.02892542, -0.04200842,  0.02270316,  0.01299495,\n",
       "         0.03675191, -0.0264742 ,  0.01844417, -0.03964479,  0.02460872,\n",
       "        -0.02527495,  0.03979373,  0.0433034 , -0.0170002 ,  0.01859173,\n",
       "         0.00591798, -0.02757793, -0.03790073, -0.03183411,  0.03713011,\n",
       "        -0.02045463,  0.01253465,  0.03721421,  0.04483048, -0.04005899,\n",
       "        -0.01912232,  0.02871368, -0.00541382,  0.0066871 ,  0.01636289,\n",
       "         0.04764027, -0.04236194, -0.01735246,  0.01287106,  0.01342117,\n",
       "        -0.00508704, -0.04558536,  0.04938788,  0.03966496,  0.01645186,\n",
       "        -0.02751919, -0.00025268,  0.04946247,  0.0231194 , -0.04224833,\n",
       "        -0.02562729,  0.00840534,  0.01882431, -0.01196084,  0.04738503,\n",
       "        -0.01595874,  0.0392956 , -0.04264091,  0.03052733,  0.04476449,\n",
       "         0.04667583,  0.04814129,  0.0459304 , -0.04918369, -0.02801726,\n",
       "        -0.03132471, -0.0090196 ,  0.02798616,  0.03660127, -0.03296532,\n",
       "         0.00137492, -0.03357633, -0.02985814,  0.00114782, -0.03376057,\n",
       "        -0.00770432,  0.02774337,  0.00679234,  0.03784313, -0.030425  ,\n",
       "        -0.00408318, -0.02842802, -0.04302514,  0.00228058,  0.03637588,\n",
       "        -0.02432981,  0.02465888,  0.03871601, -0.00490255,  0.02831242,\n",
       "         0.02871419, -0.00748217, -0.01316854, -0.04007083,  0.00194478,\n",
       "         0.0486904 , -0.02967325, -0.01296461], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 'Where will the winds take my gypsy blood this time? http://t.co/66YVulIZbk')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06be48a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset with running a series of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c632",
   "metadata": {},
   "source": [
    "There are some Model to learn text:\n",
    "\n",
    "0, Naive Bayes with TF-IDF encoder (baseline)\n",
    "\n",
    "1, Feed-forward neural network (dence model)\n",
    "\n",
    "2, LSTM (RNN)\n",
    "\n",
    "3, GRU (RNN)\n",
    "\n",
    "4, Bidirectional-LSTM (RNN)\n",
    "\n",
    "5, 1D Convolutional Neural Network\n",
    "\n",
    "6, TensorFlow Hub Pretrained Feature Extractor\n",
    "\n",
    "7, TensorFlow Hub Pretrained Feature Extractor (10% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ceadf",
   "metadata": {},
   "source": [
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modeling with tensorflow:\n",
    "\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f8e51",
   "metadata": {},
   "source": [
    "# Create GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137b60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text2vec(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(64)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea54500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 15)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde916ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d9dfbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback ( need to a new one for each model)\n",
    "from helper_function import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14ec9c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_3_GRU/20211229-184431\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 23ms/step - loss: 0.5275 - accuracy: 0.7262 - val_loss: 0.4458 - val_accuracy: 0.8031\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.3214 - accuracy: 0.8645 - val_loss: 0.4449 - val_accuracy: 0.7953\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.2215 - accuracy: 0.9167 - val_loss: 0.5017 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1577 - accuracy: 0.9463 - val_loss: 0.6222 - val_accuracy: 0.7861\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1216 - accuracy: 0.9593 - val_loss: 0.6381 - val_accuracy: 0.7795\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_3_history = model_3.fit(x=train_sentences,\n",
    "                             y=train_lables,\n",
    "                             epochs=5,\n",
    "                             validation_data=(val_sentences, val_lables),\n",
    "                             callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                   experiment_name=\"model_3_GRU\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "607c21eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9983696 ],\n",
       "       [0.785221  ],\n",
       "       [0.00168416],\n",
       "       [0.43720007],\n",
       "       [0.9947716 ],\n",
       "       [0.77878106],\n",
       "       [0.8552263 ],\n",
       "       [0.14568129],\n",
       "       [0.28284764],\n",
       "       [0.0279229 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction of GRU\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d01359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 0., 0., 1., 1., 1., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Model 3 prediction to label format \n",
    "model_3_pred = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "631382a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.95275590551181,\n",
       " 'prediction': 0.7789333564214768,\n",
       " 'recall': 0.7795275590551181,\n",
       " 'f1': 0.7776283995045866}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 3 results\n",
    "\n",
    "from Evaluation import caluculate_results\n",
    "model_3_results = caluculate_results(y_true=val_lables,\n",
    "                                    y_pre=model_3_pred)\n",
    "model_3_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
