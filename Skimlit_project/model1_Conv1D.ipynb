{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d4c1f7",
   "metadata": {},
   "source": [
    "# Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d71da6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from helper_function import calculate_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38069d",
   "metadata": {},
   "source": [
    "Read csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8aed2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"dataset/csv/train.csv\")\n",
    "test_data = pd.read_csv(\"dataset/csv/test.csv\")\n",
    "val_data = pd.read_csv(\"dataset/csv/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439149d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>line_number</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24464531</td>\n",
       "      <td>0</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of 6 weeks of dail...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24464531</td>\n",
       "      <td>1</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of 125 patients with primary knee OA w...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24464531</td>\n",
       "      <td>2</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24464531</td>\n",
       "      <td>3</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24464531</td>\n",
       "      <td>4</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>24464531</td>\n",
       "      <td>7</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>For the absolute change in percent atheroma vo...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>24464531</td>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>For PAV , a significantly greater percentage o...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>24464531</td>\n",
       "      <td>9</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Both strategies had acceptable side effect pro...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>24464531</td>\n",
       "      <td>10</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>Compared with standard statin monotherapy , th...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>24464531</td>\n",
       "      <td>11</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>( Plaque Regression With Cholesterol Absorptio...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  line_number discourse_type  \\\n",
       "0       24464531            0      OBJECTIVE   \n",
       "1       24464531            1        METHODS   \n",
       "2       24464531            2        METHODS   \n",
       "3       24464531            3        METHODS   \n",
       "4       24464531            4        METHODS   \n",
       "...          ...          ...            ...   \n",
       "180035  24464531            7        RESULTS   \n",
       "180036  24464531            8        RESULTS   \n",
       "180037  24464531            9        RESULTS   \n",
       "180038  24464531           10    CONCLUSIONS   \n",
       "180039  24464531           11    CONCLUSIONS   \n",
       "\n",
       "                                           discourse_text  total_lines  \n",
       "0       To investigate the efficacy of 6 weeks of dail...           12  \n",
       "1       A total of 125 patients with primary knee OA w...           12  \n",
       "2       Outcome measures included pain reduction and i...           12  \n",
       "3       Pain was assessed using the visual analog pain...           12  \n",
       "4       Secondary outcome measures included the Wester...           12  \n",
       "...                                                   ...          ...  \n",
       "180035  For the absolute change in percent atheroma vo...           12  \n",
       "180036  For PAV , a significantly greater percentage o...           12  \n",
       "180037  Both strategies had acceptable side effect pro...           12  \n",
       "180038  Compared with standard statin monotherapy , th...           12  \n",
       "180039  ( Plaque Regression With Cholesterol Absorptio...           12  \n",
       "\n",
       "[180040 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331a4b2",
   "metadata": {},
   "source": [
    "Turn into text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6caad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_data[\"discourse_text\"]\n",
    "test_sentences = test_data[\"discourse_text\"]\n",
    "val_sentences = val_data[\"discourse_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf86bb",
   "metadata": {},
   "source": [
    "## Create text vectorizer layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f7292",
   "metadata": {},
   "source": [
    "### turn vocab into tokenaization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f771396",
   "metadata": {},
   "source": [
    "See how many words are in our vocab\n",
    "\n",
    "( taken from table 2 in: https://arxiv.org/pdf/1710.06071/pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28d244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 68000  # the amount of words in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb6d63",
   "metadata": {},
   "source": [
    "Create text vectorizer\n",
    "\n",
    "Plese refer the output_sequence_length in https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/09_SkimLit_nlp_milestone_project_2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61898324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = max_tokens\n",
    "                                   , output_sequence_length = 55) # this is kind of magic number. See the github later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75261e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b33505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Intragroup comparisons showed that CERO ( 2 ) increased by 14.3 % ( P = 0.009 , LR group ) and 13.2 % ( P = 0.032 , HES group ) , respectively , and SjvO ( 2 ) was decreased by 8.8 % ( P = 0.016 , LR group ) and 8.1 % ( P = 0.026 , HES group ) , respectively , after tumor removal , compared with baseline .\n",
      "\n",
      "Length of text: 307\n",
      "\n",
      "Vectorized text: [[ 7384  1444   158    28 30721    32   101    22  2149    14  2205  3277\n",
      "     13     3  2167    14  5243  3953    13    86     3 28024    32    10\n",
      "    218    22  1166    14  3189  3277    13     3  1017    14  4575  3953\n",
      "     13    86    21   789  1400    34     7    51     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Test our text vectorizer on randome sentences\n",
    "import random\n",
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence)}\")\n",
    "print(f\"\\nVectorized text: {text_vectorizer([target_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd52ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 68000\n",
      "Most common words in the vocab: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words in the vocab: ['httpsuploaduminacjpcgiopenbinctrctrcgifunctionbrowsactionbrowstypesummaryrecptnor000008238languagee', 'httpsregisterclinicaltrialsgov', 'httpsmartmicrosurgerycom', 'httpseudractemaeuropaeuindexhtml', 'httpseudractemaeuropaeu']\n"
     ]
    }
   ],
   "source": [
    "# How many words in our training vocablary\n",
    "\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
    "print(f\"Most common words in the vocab: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words in the vocab: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e0cf15",
   "metadata": {},
   "source": [
    "Get config of our text vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14b3a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None, None),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca83dc6",
   "metadata": {},
   "source": [
    "### turn vocab numbers into embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f560b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = layers.Embedding(input_dim = 68000 # the size of input shape: the size of vocab in our data\n",
    "                            ,output_dim = 128 # the output shape\n",
    "                            ,mask_zero = True # use masking to handle variable\n",
    "                            ,name=\"tokes_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c13e46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Following adjustment , mean total time was reduced 8.71 ( 95 % CI = 5.15-12 .26 ) minutes ( adjusted P < 0.001 ) in Restart participants and 2.31 ( -2.19 to 6.81 ) minutes ( adjusted P = 0.472 ) in New Start participants receiving video counseling .\n",
      "tf.Tensor(\n",
      "[[[-0.01454742 -0.04727423  0.02528096 ... -0.02653695  0.00135207\n",
      "   -0.04716783]\n",
      "  [-0.04480843 -0.00294306 -0.01004447 ... -0.01625295  0.01970846\n",
      "   -0.04635986]\n",
      "  [-0.01734294  0.01029035  0.01644957 ...  0.03022437 -0.01106698\n",
      "   -0.04460226]\n",
      "  ...\n",
      "  [-0.04470002  0.04140927  0.03871037 ... -0.03882939 -0.03799467\n",
      "   -0.03720605]\n",
      "  [-0.04470002  0.04140927  0.03871037 ... -0.03882939 -0.03799467\n",
      "   -0.03720605]\n",
      "  [-0.04470002  0.04140927  0.03871037 ... -0.03882939 -0.03799467\n",
      "   -0.03720605]]], shape=(1, 55, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Show example embedding\n",
    "target_original_text = random.choice(train_sentences)\n",
    "target_vec_text = text_vectorizer([target_original_text])\n",
    "print(f\"Original text: {target_original_text}\")\n",
    "print(embedding(target_vec_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e13916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'tokes_embedding',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None, None),\n",
       " 'dtype': 'float32',\n",
       " 'input_dim': 68000,\n",
       " 'output_dim': 128,\n",
       " 'embeddings_initializer': {'class_name': 'RandomUniform',\n",
       "  'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}},\n",
       " 'embeddings_regularizer': None,\n",
       " 'activity_regularizer': None,\n",
       " 'embeddings_constraint': None,\n",
       " 'mask_zero': True,\n",
       " 'input_length': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c3c84",
   "metadata": {},
   "source": [
    "## Make numeric lables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb312a",
   "metadata": {},
   "source": [
    "Make one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccd426a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_lables_one_hot = one_hot_encoder.fit_transform(train_data[\"discourse_type\"].to_numpy().reshape(-1,1))\n",
    "test_lables_one_hot = one_hot_encoder.fit_transform(test_data[\"discourse_type\"].to_numpy().reshape(-1,1))\n",
    "val_lables_one_hot = one_hot_encoder.fit_transform(val_data[\"discourse_type\"].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b80640d",
   "metadata": {},
   "source": [
    "Make labels encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7d690c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_data[\"discourse_type\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.fit_transform(test_data[\"discourse_type\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.fit_transform(val_data[\"discourse_type\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69c63f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(label_encoder.classes_)\n",
    "num_class, label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174d785",
   "metadata": {},
   "source": [
    "## Create datasets with tf.data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9190ec3",
   "metadata": {},
   "source": [
    "The main steps we use with our data is to turn it into `PrefetchDataset` of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e38ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our data into Tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_lables_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_lables_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_lables_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82108ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take TensorflowSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8f0e60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)>,\n",
       " <PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)>,\n",
       " <PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67380011",
   "metadata": {},
   "source": [
    "## Create Simple Conv1D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e063dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape = (1,), dtype = tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(filters = 64, kernel_size = 5, activation = \"relu\", padding = \"same\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_class, activation = \"softmax\")(x)\n",
    "model1_Conv1D = tf.keras.Model(inputs, outputs, name=\"model1_Conv1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e71e1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model1_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " tokes_embedding (Embedding)  (None, 55, 128)          8704000   \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,745,349\n",
      "Trainable params: 8,745,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1_Conv1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6531e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model1_Conv1D.compile(loss=\"categorical_crossentropy\",\n",
    "                     optimizer=tf.keras.optimizers.Adam(),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd673b30",
   "metadata": {},
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39b64a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 78s 135ms/step - loss: 0.9030 - accuracy: 0.6434 - val_loss: 0.6759 - val_accuracy: 0.7440\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 80s 143ms/step - loss: 0.6485 - accuracy: 0.7575 - val_loss: 0.6116 - val_accuracy: 0.7733\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 75s 133ms/step - loss: 0.6076 - accuracy: 0.7775 - val_loss: 0.5860 - val_accuracy: 0.7866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17f26e554f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_Conv1D.fit( train_dataset,\n",
    "                  steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                  epochs=3,\n",
    "                  validation_data = val_dataset,\n",
    "                  validation_steps = int(0.1 * len(val_dataset))\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "646a7088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 6s 6ms/step - loss: 0.5853 - accuracy: 0.7899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.585313618183136, 0.7899178862571716]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "model1_Conv1D.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aef4688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_pred_probs = model1_Conv1D.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a1079cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4027787e-01, 1.4899255e-01, 8.2412697e-02, 3.0343747e-01,\n",
       "        2.4879478e-02],\n",
       "       [4.3888324e-01, 2.7221543e-01, 1.5566239e-02, 2.6431456e-01,\n",
       "        9.0205297e-03],\n",
       "       [1.9784681e-01, 5.2143876e-03, 1.5086071e-03, 7.9539645e-01,\n",
       "        3.3762430e-05],\n",
       "       ...,\n",
       "       [4.3376230e-07, 9.0905094e-05, 8.0912217e-04, 2.6279827e-07,\n",
       "        9.9909925e-01],\n",
       "       [1.9595772e-02, 4.2454383e-01, 7.0767745e-02, 2.4107980e-02,\n",
       "        4.6098468e-01],\n",
       "       [2.1675684e-01, 5.0665849e-01, 8.1012905e-02, 5.9875127e-02,\n",
       "        1.3569674e-01]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50e11d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pred probs into classes\n",
    "model1_preds = tf.argmax(model1_pred_probs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1953fd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 4, 1], dtype=int64)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28995734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_results = calculate_results(y_true = val_labels_encoded,\n",
    "                                  y_pred = model1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "456a5203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.99179134118893,\n",
       " 'precision': 0.7842417701717078,\n",
       " 'recall': 0.7899179134118893,\n",
       " 'f1': 0.7862287791407456}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
